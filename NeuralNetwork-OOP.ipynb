{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>This is by no means a tutorial on machine learning , its rather for those who already have their hands on on implementing neural network models using Frameworks like TensorFlow , Caffe , Keras etc.. but wanting to explore what's happening under the hood. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cycle of Artificial Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a supervised learning, training a machine learning model follows a chronological sequence of steps , from feeding a set of labeled instances , calculate a loss function , reduce the loss through an iterative process untill convergence. Neural network is no exception, however associated acronyms are adopted to define each step :\n",
    "<ul> \n",
    "    <li>Feed Forward</li>\n",
    "    <li>Loss Function</li>\n",
    "    <li>Backpropogation</li>\n",
    "    <li>Convergence</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational graph of a logistic regression as below consists of 2 features x1 and x2 along with their respective weights and a bias :\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "z= w1x1 + w2x2 + b \\qquad  a = \\hat{y}=\\sigma(z) \\qquad where \\qquad \\sigma(z) = \\frac{1}{1+e^{-z}} = (1 + e^{-z})^{-1} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random weights w1 and w2 are assigned to x1 and x2 respectively. In a binary classification we shall expect an output between 0 or 1. to guarantee such output, Sigmoid function is applied and the result is a predicted output ($\\hat{y}$) which , most likely deviate from the actual output (y). This deviation is the error (aka loss function) that needs to be corrected. Gradient descent is then performed to reduce this error by tuning the parameters w1 , w2 and b . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's a cost function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume for example you want to predict the value of your old laptop. for simplicity, we assume that the only factor that affects the value of your laptop is its CPU clock speed. You manage to collect information on already sold laptops on ebay where you know their CPU clock speed (x) and the value they were sold at (y).\n",
    "Mathematically , this is a linear function as : Value = ratio * speed , or more formally h(x)=y= $\\theta$.x <br>\n",
    "To train a model that can predict the value of my laptop, we have taken all this instances, randomy assiging a ratio($\\theta$) and we calculated an predicted output $\\hat{y}$ for each example. with this random initialization, we are going to end up with a difference between the predicted value and the real value. this difference is the error, and the cost function is the average sum of all errors happened at each example defined as :\n",
    "\\begin{align}\n",
    "\\ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m}(\\hat{y} - y)^2\n",
    "\\end{align}\n",
    "###  <i> Why not using the Average Sum of Squares for Logistic Regression ? :</i>\n",
    "if we were to plot a graph of this cost against the predicted value for Linear regression, we are going to end up in a convex shape [picture A], and that is good enough for an error decreasing mechanism, we talk about later. That's not the case with Logistic Regression, where the predicted output is passed to a more complex function ( the sigmoid ) which, if using the Sum of Squared Error cost function, we will end up in a non-convex shape [picture B] . What this is even important ? that's because our goal is go down those hills to find the lowest point. in the case of non-linearity, there is no guarantee we will reach the lowest point as we have many of those ( called local optima ). for this reason , a better way to write a cost function for Logistic Regression that give us a convex shape where we the goal is to reach the lowed point is not confusion ( global minimum )  : \n",
    "\\begin{align}\n",
    "\\mathscr{L}(\\hat{y},y)=-ylog(\\hat{y})-(1-y)log(1-\\hat{y})\n",
    "\\end{align}\n",
    "\n",
    "i have talked about a mechanism that mathemitically can help us decrease the cost . This mechanism is called Gradient descent. Before diving into it, let's refresh on some calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/non-convex-convex.jpg\" width=\"700\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus Is Easier Without its Naming Convention Terms :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial terror, which choked off most of us at school, are those acronyms used in calculus such \n",
    "as \"an element of\" (dx), integral $\\int$dx. it turns out that \"an element of\" (dx) is simply \"a litte bit\" of x , a really tiny bit , and the $\\int$dx is the sum of those little bits. as simple as this ! <br>\n",
    "More concretely, let's say we have two correlated variable x and y , and we need to measure how much y will change when we nudge x a tiny bit ( dx ). Assume y = 2x , by nudging x with a very tiny value ( dx = 0.001 ) we have an alteration of y by (dy)  : <br>\n",
    "\\begin{align}\n",
    "\\ y + dy = 2(x + dx)\n",
    "\\end{align}\n",
    "if x= 3 , then : \n",
    "\\begin{align}\n",
    "\\ y = 2(3) = 6 \\\\\n",
    "\\ y=2(3+0.001) = 6.002 \\\\ dy = 0.002\n",
    "\\end{align}\n",
    "Thus , we can say , with every nudge of x (dx) , y is changing twice as much as (dx). we've done all this as we are after the value of such ratio 0.002/0.001  = 2\n",
    "\\begin{align}\n",
    "\\ \\frac{dy}{dx} = 2 \n",
    "\\end{align}\n",
    "and that is the partial derivative !\n",
    "\n",
    "In the above example we have seen a positive correlation but that's no always the case. let's take another example where an increase in x causes a decrease in y, and we will go a bit deeper to see, geometrically, how this change is calculated.\n",
    "in the below picture,a ladder laid on a wall , assuming that AB is a fixed length. let x be the horizontal distance from a wall of the bottom end of the ladder and y the height it reaches up the wall. \n",
    "if pull its lower end by a value (dx) by 1 cm , the height of the ladder will obviously decrease by (dy). but by how much ? \n",
    "if we calculate the y by euclid , we shall be able to figure out the value of dy . the length of the ladder : \n",
    "\\begin{align}\n",
    "\\ \\sqrt{(180)^2+(19)^2} = 181 cm\n",
    "\\end{align}\n",
    "and then the new height would be : \n",
    "\\begin{align}\n",
    "\\ (y - dy)^2 = (181)^2 - (20)^2 = 32361 \\\\\n",
    "\\ y - dy = \\sqrt{32361} = 179.89\n",
    "\\end{align}\n",
    "now y is 180 , so dy is 180 - 179.89 = 0.11 cm . so we see by making an increase of 1 cm ( dx = 1 ) has resulted in a decrease of dy by 0.11 cm , thus : \n",
    "\\begin{align}\n",
    "\\ \\frac{dy}{dx}= -\\frac{0.11}{1}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ladder-calc.jpg\" width=\"400\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this explanation makes the part of calculus we need is easier. and that's exactly the core concept of how gradient descent works, which we will explore next . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have talked in the Loss function section, that there is mechanism that helps us decrease our errors to the maximum possible. if we have to plot the cost function against one variable, we end up in a convex shape as in the below graph. let's talk about possible scenarios W can take, one at point \"P\" and point another at point \"N\". \n",
    "Calculus tells us, that the derivative of a function is the slope of that function. at point P the derivative (slope) is positive we need decrease W, hoping we reach the least error possible \"G\". on the other hand if the derivative (slope) is negative as at point \"N\" , then we need to decrease the value of W, again hoping to reach the global minimum as \"G\". However, we need to choose a rate of such increase or decrease. and this is conventially a hyperparameter called alpha.\n",
    "Technically, at each iteration of gradient descent we need to update W as : \n",
    "\\begin{align}\n",
    "\\ w= w - \\alpha \\frac{dl}{dw}\n",
    "\\end{align}\n",
    "dl/dw is the derivative of l in respect to w . in case that derivate is positive then w - $\\alpha$  ( <i>positive value</i> ) is a decrease of w and in case the derivative is negative then w -  $\\alpha (<i>negative value</i> is a decrease of w , and that's exactly what we aimed for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/loss-function3.jpg\" width=\"600\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this Cost Function and Gradient Descent relates to Neural Network ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of a neural network as a group of Logistic Regression entities,whereas the output of one Logistic Regression is an input to another. Regardless of the functions used between a Neural Network's input and its output , there will errors caused by the random initialization of parameters, at least in the first pass ( feedforward ). Those errors need to propogate back to the input level and that's another chain of gradient descents ( backpropagation ).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now given that you are familiar with the core parts, we will take a common simple example on how to train a neural network. the input data consists only of 4 instances along with its 4 outputs as below : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dataset.jpg\" width=\"300\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can from the below graph, instances are stacked vertically, so the number of rows are the number of features ( x1,x2 and x3 ) , whereas the columns are the number of instances ( in our case 4 instances )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nn.png\" width=\"900\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pass from left to right, more specifically, calculated weighted features, which resulted in a predicted value $\\hat{y}$ to be compared with the actual output is conventionally known as the \"feed forwards\" pass. below is the computational graph for our example . Note here Capital W , Z , A is being used due to vectorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/feedforward-comp-graph.png\" width=\"900\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the loss function is calculated, the resulting error is propagated back to the input level. to get the right weights' values that decrease my loss function, we need first to see how much a variation in the cost function results in variation to its previous step a2 ( dloss/da2 ) and how much variation is a2 can result in variation to z2 (da2/dz2). This process is not different from the one in logistic regression, except that we have more layers prior to the output, thus many derivatives. Chain rule in calculsus tells if a affects b and b affects c , then a affects c by the product of the two derivatives (dc/da = db/da * dc/db ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backprop-comp-graph.png\" width=\"1000\" /> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the algorithm to implement backpropogation is as below : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "&dZ^{[2]} = A^{[2]} - Y \\\\\n",
    "&dW^{[2]} =  \\frac{1}{m} dZ^{[2]} A^{[1]^{T}} \\\\\n",
    "&db^{[2]} = \\frac{1}{m}dZ^{[2]} \\\\\n",
    "&dZ^{[1]} = W^{[2]^{T}} dZ^{[2]} \\cdot g\\prime(Z^{[1]}) \\\\\n",
    "&dW^{[1]} = \\frac{1}{m}dZ^{[1]} X^{T} \\\\\n",
    "&db^{[1]} = \\frac{1}{m}dZ^{[1]} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "&\\mathscr{L}(\\hat{y},y)=-ylog(\\hat{y})-(1-y)log(1-\\hat{y}) \\\\\n",
    "&\\frac{d\\mathscr{L}(\\hat{y},y)}{d\\hat{y}}\\ = - \\bigg( \\bigl( \\frac{y}{\\hat{y}}\\bigr) + (1-y)(-\\frac{1}{1-\\hat{y}} ) \\bigg) \\\\\n",
    "&\\frac{d\\mathscr{L}(\\hat{y},y)}{d\\hat{y}}\\ = - \\frac{y}{\\hat{y}} + \\frac{(1-y)}{(1-\\hat{y})} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "&\\sigma(z) = \\frac{1}{1+e^{-z}} = (1 + e^{-z})^{-1} \\\\\n",
    "&\\sigma(z)\\prime = (1 + e^{-z})^{-1} = - \\frac{(1+e^{-z})\\prime} {(1+e^{-z})^{2}} =-(1+e^{-z})^{-2}(1+e^{-z})\\prime \\\\\n",
    "&\\sigma(z)\\prime =-(1+e^{-z})^{-2}(e^{-z} (-1)) = (1+e^{-z})^{-2} (e^{-z}) = \\frac{e^{-z}}{(1+e^{-z})^{2}} \\\\\n",
    "&\\sigma(z)\\prime = \\frac{1}{(1+ e^{-z})}  \\frac{e^{-z}}{(1+e^{-z})} \\\\\n",
    "&\\sigma(z)\\prime = \\frac{1}{(1+ e^{-z})}  \\frac{(e^{-z}+1-1)}{(1+e^{-z})} \\\\\n",
    "&\\sigma(z)\\prime = \\frac{1}{(1+ e^{-z})}  \\bigg(\\frac{(1+e^{-z})}{(1+e^{-z})} - \\frac{-1}{(1+e^{-z})} \\bigg) \\\\\n",
    "&\\sigma(z)\\prime = \\frac{1}{(1+ e^{-z})}  \\bigg( 1 - \\frac{-1}{(1+e^{-z})} \\bigg) \\\\\n",
    "&\\sigma(z)\\prime = \\sigma(z) \\space \\big( 1 - \\sigma(z)\\big) \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Example Code (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__ (self,X,Y,hidden_layer_neurons,learning_rate,num_of_iters):\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        self.hidden=hidden_layer_neurons\n",
    "        self.lr = learning_rate\n",
    "        self.iters=num_of_iters\n",
    "        self.m=Y.shape[1]\n",
    "        self.W1=np.random.rand(self.hidden,X.shape[0]) *0.01\n",
    "        self.W2=np.random.rand(self.Y.shape[0],self.hidden) *0.01\n",
    "        self.b1=np.zeros(self.hidden).reshape(self.hidden,1)\n",
    "        self.b2=np.zeros(Y.shape[0]).reshape(Y.shape[0],1)\n",
    "        self.params={'W1':self.W1,'b1':self.b1,'W2':self.W2,'b2':self.b2}\n",
    "        #        print ('Shape of X {}\\nShape of Y {}\\nShape of W1 {}\\nShape of W2 {}\\nShape of b1 {}\\nShape of b2 {}'.format(self.X.shape,self.Y.shape,self.W1.shape,self.W2.shape,self.b1.shape,self.b2.shape))\n",
    "        \n",
    "    \n",
    "    def sigmoid (self,z):\n",
    "        return 1 / ( 1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self,z):\n",
    "        f= 1 / ( 1 + np.exp(-z))\n",
    "        return f * (1 - f)\n",
    "    \n",
    "    def ReLU(self,x):\n",
    "        return x * (x > 0)\n",
    "\n",
    "    def dReLU(self,x):\n",
    "        return 1. * (x > 0)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def feedfoward(self,params):\n",
    "        z1 = np.dot(params['W1'],self.X) + params['b1']\n",
    "        a1 = self.sigmoid (z1)\n",
    "        #a1 = self.ReLU (z1)\n",
    "        z2 = np.dot (params['W2'],a1) + params['b2']\n",
    "        a2 = self.sigmoid (z2)\n",
    "        return {'z1':z1,'a1':a1,'z2':z2,'a2':a2}\n",
    "        \n",
    "    def cost(self,Yhat, Y):\n",
    "        cost__ = -np.sum(np.multiply(np.log(Yhat), Y) + np.multiply((1 - Y), np.log(1 - Yhat)))/self.m\n",
    "        return np.squeeze(cost__)\n",
    "    \n",
    "    def backpropogation(self,cache,params):\n",
    "        dz2 = cache['a2'] - self.Y\n",
    "        dW2 = 1/self.m * (np.dot (dz2,cache['a1'].T ) ) \n",
    "        db2 = 1/self.m * np.sum( dz2 , axis=1 , keepdims=True)\n",
    "        dz1 = np.dot (params['W2'].T,dz2) * self.sigmoid_derivative (cache['z1'])\n",
    "        #dz1 = np.dot (params['W2'].T,dz2) * self.dReLU (cache['z1'])\n",
    "        dW1 = 1/self.m * ( np.dot (dz1,self.X.T))\n",
    "        db1 = 1/self.m * np.sum(dz1 ,axis = 1 , keepdims=True)\n",
    "        return {'dW1':dW1,'db1':db1,'dW2':dW2,'db2':db2}\n",
    "        \n",
    "    def updateParams(self,dparams,params):\n",
    "        W1 =  params['W1'] - (self.lr * dparams['dW1'] )\n",
    "        b1 = params['b1'] - ( self.lr * dparams['db1'])\n",
    "        W2 = params['W2'] - ( self.lr * dparams['dW2'])\n",
    "        b2 = params['b2'] - ( self.lr * dparams['db2'])\n",
    "        return {'W1':W1,'b1':b1,'W2':W2,'b2':b2}\n",
    "        \n",
    "    def fit(self):\n",
    "        cost_ = []\n",
    "        params=self.params\n",
    "        for i in range(self.iters):\n",
    "            cache =self.feedfoward(params)\n",
    "            self.cost(cache['a2'] , self.Y)\n",
    "            dparams=self.backpropogation(cache,params)\n",
    "            params=self.updateParams(dparams,params)\n",
    "            cost_.append(self.cost(cache['a2'],self.Y))\n",
    "        return cost_ ,params\n",
    "    \n",
    "    def predict(self,input_val,params):\n",
    "        z1 = np.dot(params['W1'],input_val) + params['b1']\n",
    "        a1 = self.sigmoid (z1)\n",
    "        z2 = np.dot (params['W2'],a1) + params['b2']\n",
    "        a2 = self.sigmoid (z2)\n",
    "        return a2\n",
    "        \n",
    "\n",
    "ds= np.array ( [[0,0,1,0],\n",
    "               [0,1,1,1],\n",
    "               [1,0,1,1],\n",
    "               [1,1,1,0]])\n",
    "df = pd.DataFrame (ds , columns=['x1','x2','x3','Y'])\n",
    "X = df.loc[:,df.columns!='Y']\n",
    "X = X.T\n",
    "Y= df['Y'].values.reshape(1,4)\n",
    "    \n",
    "nn= NeuralNetwork(X,Y,4,4.7,900)    \n",
    "\n",
    "mycost ,params= nn.fit()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook \n",
    "plt.plot(list(range(900)), mycost, '-r')\n",
    "   \n",
    "nn.predict(np.array([[1,1,1]]).T,params)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradient-loss-for-nn-example.png\" width=\"500\" /> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
